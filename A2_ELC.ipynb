{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2 ELC ",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is how the code works:\n",
        "\n",
        "1. Convert paragraph into sentences metrics.\n",
        "2. Cleaning the data.\n",
        "3. Conversion of sentences to corresponding word embedding\n",
        "4. Conversion of question into the word embedding\n",
        "5. Apply Euclidean distance\n",
        "6. Put the results in heap with index.\n",
        "7. Pop and Print the result."
      ],
      "metadata": {
        "id": "rHNrjdK6yN2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Libraries: \n",
        "\n",
        "1. NLTK is a platform for building Python programs to work with human language data. It provides easy-to-use interfaces, text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning\n",
        "\n",
        "2. re: A regular expression or RegEx is a sequence of characters which enable you to find string or set of string using a specialized pattern.\n",
        "\n",
        "3. numpy: NumPy is a Python library used for working with arrays.\n",
        "\n",
        "4. Gensim is a free open-source Python library for representing documents as semantic vectors efficiently. It is designed to process raw, unstructured digital texts (“plain text”) using unsupervised machine learning algorithms."
      ],
      "metadata": {
        "id": "bPMMGEPFv0Qo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMt8HsfXcQ7v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import gensim\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from gensim import corpora\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "import heapq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading and reading file (6.txt)\n",
        "Note: this file is to be uploaded, and can be found in unlabelled datasets provided in the drive link as per lms"
      ],
      "metadata": {
        "id": "yxnMGok3wenq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename=\"6.txt\"\n",
        "f = open(filename, \"r\")#creating a file object\n",
        "txt=f.read() #Read the contents of the file into text\n",
        "f.close()"
      ],
      "metadata": {
        "id": "B-6fAFDkcY82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "class for preprocessing and creating word embedding\n",
        "\n",
        "here we convert the text in lower case and remove the stop words.\n",
        "\n",
        "\n",
        "Stop words can be imported using nltk packages.\n",
        "A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n",
        "\n",
        "We also perform Data Cleaning by removing the extra spaces.\n",
        "\n",
        "These \"cleaned sentences\" are stored together.\n",
        "\n",
        "TF-IDF stands for term frequency-inverse document frequency and it is a measure, used in the fields of information retrieval (IR) and machine learning, that can quantify the importance or relevance of string representations (words, phrases, lemmas, etc) in a document amongst a collection of documents\n",
        "\n",
        "it is like calculating score for each sentence as per its statistical information.\n"
      ],
      "metadata": {
        "id": "UUgUMWHbwiW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessing:\n",
        "    #constructor\n",
        "    def __init__(self,txt):\n",
        "        # Tokenization\n",
        "        nltk.download('punkt')  #punkt is nltk tokenizer \n",
        "        # breaking text to sentences\n",
        "        tokens = nltk.sent_tokenize(txt) \n",
        "        self.tokens = tokens\n",
        "        self.tfidfvectoriser=TfidfVectorizer()\n",
        "\n",
        "    def clean_sentence(self, sentence, stopwords=False):\n",
        "        sentence = sentence.lower().strip()\n",
        "        sentence = re.sub(r'[^a-z0-9\\s]', '', sentence)\n",
        "        if stopwords:\n",
        "          sentence = remove_stopwords(sentence)\n",
        "        return sentence\n",
        "\n",
        "    # store cleaned sentences to cleaned_sentences\n",
        "    def get_cleaned_sentences(self,tokens, stopwords=False):\n",
        "        cleaned_sentences = []\n",
        "        for line in tokens:\n",
        "          cleaned = self.clean_sentence(line, stopwords)\n",
        "          cleaned_sentences.append(cleaned)\n",
        "        return cleaned_sentences\n",
        "\n",
        "    #do all the cleaning\n",
        "    def cleanall(self):\n",
        "        cleaned_sentences = self.get_cleaned_sentences(self.tokens, stopwords=True)\n",
        "        cleaned_sentences_with_stopwords = self.get_cleaned_sentences(self.tokens, stopwords=False)\n",
        "        # print(cleaned_sentences)\n",
        "        # print(cleaned_sentences_with_stopwords)\n",
        "        return [cleaned_sentences,cleaned_sentences_with_stopwords]\n",
        "\n",
        "    # TF-IDF Vectorizer\n",
        "    def TFIDF(self,cleaned_sentences):\n",
        "        self.tfidfvectoriser.fit(cleaned_sentences)\n",
        "        tfidf_vectors=self.tfidfvectoriser.transform(cleaned_sentences)\n",
        "        return tfidf_vectors\n",
        "\n",
        "    #tfidf for question\n",
        "    def TFIDF_Q(self,question_to_be_cleaned):\n",
        "        tfidf_vectors=self.tfidfvectoriser.transform([question_to_be_cleaned])\n",
        "        return tfidf_vectors\n",
        "\n",
        "    # main call function\n",
        "    def doall(self):\n",
        "        cleaned_sentences, cleaned_sentences_with_stopwords = self.cleanall()\n",
        "        tfidf = self.TFIDF(cleaned_sentences)\n",
        "        return [cleaned_sentences,cleaned_sentences_with_stopwords,tfidf]\n"
      ],
      "metadata": {
        "id": "4NMN1wCPccnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a class for answering the question that we will be putting.\n",
        "\n",
        "The Parameter that we are using is the EUclidean distance."
      ],
      "metadata": {
        "id": "vvCyNDLsxxA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AnswerMe:\n",
        "    #Euclidean distance\n",
        "    def Euclidean(self, question_vector, sentence_vector):\n",
        "        vec1 = question_vector.copy()\n",
        "        vec2 = sentence_vector.copy()\n",
        "        if len(vec1)<len(vec2): vec1,vec2 = vec2,vec1\n",
        "        vec2 = np.resize(vec2,(vec1.shape[0],vec1.shape[1]))\n",
        "        return np.linalg.norm(vec1-vec2)\n",
        "\n",
        "    # main call function\n",
        "    def answer(self, question_vector, sentence_vector, method):\n",
        "        return self.Euclidean(question_vector,sentence_vector)\n"
      ],
      "metadata": {
        "id": "Hml_JX1UceFV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to retrieve the answer"
      ],
      "metadata": {
        "id": "1lSDaL1Lx879"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RetrieveAnswer(question_embedding, tfidf_vectors,method=1):\n",
        "  similarity_heap = []\n",
        "  max_similarity = float('inf')\n",
        "  \n",
        "  index_similarity = -1\n",
        "\n",
        "  for index, embedding in enumerate(tfidf_vectors):  \n",
        "    find_similarity = AnswerMe()\n",
        "    similarity = find_similarity.answer((question_embedding).toarray(),(embedding).toarray() , method).mean()\n",
        "    if method==1:\n",
        "      heapq.heappush(similarity_heap,(similarity,index))\n",
        "    else:\n",
        "      heapq.heappush(similarity_heap,(-similarity,index))\n",
        "  return similarity_heap\n"
      ],
      "metadata": {
        "id": "GwO7D5ZrcijZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The question can be put here. \n",
        "\n",
        "Some other sample questions are:\n",
        "1. What number has the global number of confirmed cases of COVID-19 has surpassed?\n",
        "\n",
        "2. What does the World Health Organization (WHO) remind all countries and communities\n",
        "\n",
        "3. How can the spread of the virus be slowed and the impact reduced?\n",
        "\n",
        "4. What does WHO call on all countries to do?\n",
        "\n",
        "5. Who is demonstrating that spread of the virus can be slowed?"
      ],
      "metadata": {
        "id": "xrbQO3LSydGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"What does WHO call on all countries to do?\"\n",
        "\n",
        "preprocess = Preprocessing(txt)\n",
        "cleaned_sentences,cleaned_sentences_with_stopwords,tfidf_vectors = preprocess.doall()\n",
        "\n",
        "question = preprocess.clean_sentence(user_question, stopwords=True)\n",
        "question_embedding = preprocess.TFIDF_Q(question)\n",
        "\n",
        "similarity_heap = RetrieveAnswer(question_embedding , tfidf_vectors ,method)\n",
        "print(\"Question: \", user_question)\n",
        "print()\n",
        "\n",
        "#we are printing just one sentence\n",
        "number_of_sentences_to_print = 1\n",
        "while number_of_sentences_to_print>0 and len(similarity_heap)>0:\n",
        "  x = similarity_heap.pop(0)\n",
        "  print(cleaned_sentences_with_stopwords[x[1]])\n",
        "  number_of_sentences_to_print-=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7yZqSLfcoGy",
        "outputId": "7d4108a5-ea15-4a42-8e29-58531349d971"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Question:  What does WHO call on all countries to do?\n",
            "\n",
            "who calls on all countries to continue efforts that have been effective in limiting the number of cases and slowing the spread of the virus\n"
          ]
        }
      ]
    }
  ]
}